{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "728736c5",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Import the following packages. To get all the packages run `pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f313e419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data processing libraries\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Environment Variables\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5505d4",
   "metadata": {},
   "source": [
    "# Scraping for Max Page Number\n",
    "\n",
    "Goal of this section is to obtain a dictionary of all states and their respective max number of pages found on their search page in the *Find Locations* section of the Jersey Mikes website. This provides us an estimation/metric of how many stores are in a state or how many addresses to scrape for later on.\n",
    "\n",
    "Base URL: https://www.jerseymikes.com/locations/{state_abriev}?page={page_num}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31c9d73c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AL': 3,\n",
       " 'AK': 1,\n",
       " 'AZ': 7,\n",
       " 'AR': 1,\n",
       " 'CA': 29,\n",
       " 'CO': 5,\n",
       " 'CT': 3,\n",
       " 'DE': 1,\n",
       " 'DC': 1,\n",
       " 'FL': 18,\n",
       " 'GA': 9,\n",
       " 'HI': 1,\n",
       " 'ID': 1,\n",
       " 'IL': 8,\n",
       " 'IN': 2,\n",
       " 'IA': 2,\n",
       " 'KS': 1,\n",
       " 'KY': 2,\n",
       " 'LA': 1,\n",
       " 'ME': 1,\n",
       " 'MD': 4,\n",
       " 'MA': 4,\n",
       " 'MI': 6,\n",
       " 'MN': 4,\n",
       " 'MS': 1,\n",
       " 'MO': 2,\n",
       " 'MT': 1,\n",
       " 'NE': 1,\n",
       " 'NV': 2,\n",
       " 'NH': 1,\n",
       " 'NJ': 11,\n",
       " 'NM': 1,\n",
       " 'NY': 7,\n",
       " 'NC': 16,\n",
       " 'ND': 1,\n",
       " 'OH': 8,\n",
       " 'OK': 2,\n",
       " 'OR': 3,\n",
       " 'PA': 6,\n",
       " 'RI': 1,\n",
       " 'SC': 7,\n",
       " 'SD': 1,\n",
       " 'TN': 5,\n",
       " 'TX': 17,\n",
       " 'UT': 3,\n",
       " 'VT': 1,\n",
       " 'VA': 8,\n",
       " 'WA': 5,\n",
       " 'WV': 1,\n",
       " 'WI': 3,\n",
       " 'WY': 1}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base url for finding store by state\n",
    "base_url = 'https://www.jerseymikes.com/locations/'\n",
    "page_url = '?page='\n",
    "\n",
    "# Filter for states by number of pages in store link\n",
    "states_counts = {}\n",
    "states_abriev = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'DC', 'FL', 'GA',\n",
    "                 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', 'MA',\n",
    "                 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY',\n",
    "                 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX',\n",
    "                 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']\n",
    "\n",
    "# Scrap for \n",
    "for state in states_abriev:\n",
    "    # Choose min page number, to find any higher page\n",
    "    state_url = base_url + state + page_url + '1'\n",
    "    \n",
    "    # Create Beautiful Soup object to parse html content\n",
    "    document_html = requests.get(state_url).text\n",
    "    soup = BeautifulSoup(document_html, 'html.parser')\n",
    "\n",
    "    # Parse page links\n",
    "    states_content = soup.find('ul', class_='pagination')\n",
    "    \n",
    "    # No additional pages mean max page of 1\n",
    "    max_page = 1\n",
    "    if states_content:\n",
    "        states_content = [i.text.strip() for i in states_content.find_all('a')]\n",
    "        states_content = [int(i) if i.isnumeric() else 0 for i in states_content]\n",
    "        max_page = max(states_content)\n",
    "\n",
    "    states_counts.update({state: max_page})        \n",
    "\n",
    "states_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdee26f3",
   "metadata": {},
   "source": [
    "# Scraping for Addresses by State\n",
    "\n",
    "Goal of this section is to obtain the addresses of all stores for every Jersey Mikes state locations pages we mapped earlier.\n",
    "\n",
    "Base URL: https://www.jerseymikes.com/locations/{state_abriev}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe9336e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape addresses for each state, search through all pages for each state\n",
    "locations = []\n",
    "base_url = 'https://www.jerseymikes.com/locations/'\n",
    "page_url = '?page='\n",
    "\n",
    "for state, page in states_counts.items():\n",
    "    # Create new soup from current page\n",
    "    state_url = base_url + state + page_url\n",
    "    \n",
    "    for page in range(1, page+1):\n",
    "        state_url += str(page)\n",
    "        document_html = requests.get(state_url).text\n",
    "        soup = BeautifulSoup(document_html, 'html.parser')\n",
    "        soup = soup.find_all('p', itemprop='address')\n",
    "        locations += soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2f5579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse addresses\n",
    "for index, location in enumerate(locations):\n",
    "    address = location.text.strip()\n",
    "    end = address.find('(')\n",
    "    address = address[:end]\n",
    "    address = address.replace('\\n', ',').strip()\n",
    "    locations[index] = address + 'US'\n",
    "    \n",
    "locations = set(locations)\n",
    "locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c120fe9e",
   "metadata": {},
   "source": [
    "# Obtaining Geocode from Google Maps API\n",
    "\n",
    "Use Google Maps Platform to obtain longitude and latitude from addresses. Obtain an API key here, https://developers.google.com/maps/documentation/javascript/get-api-key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "151cea9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key\n",
    "load_dotenv()\n",
    "api_key = os.getenv('GOOGLE_MAPS_API_KEY')\n",
    "\n",
    "# Create empty dataframe\n",
    "df = pd.DataFrame(columns=['Address', 'Latitude', 'Longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e2af5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Address</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Imperial Promenade,5675 E La Palma Avenue,Suit...</td>\n",
       "      <td>33.860970</td>\n",
       "      <td>-117.791142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4509 Phoenix Avenue,Fort Smith, AR 72903-6005,US</td>\n",
       "      <td>35.338555</td>\n",
       "      <td>-94.383186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3821 Lakewood Boulevard ,Ste. 101,Long Beach, ...</td>\n",
       "      <td>33.828598</td>\n",
       "      <td>-118.143035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Station Park West,1060 West Park Lane,Suite 11...</td>\n",
       "      <td>40.983426</td>\n",
       "      <td>-111.907794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6095 Carlson Way,Suite B,Marion, IA 52302-6651,US</td>\n",
       "      <td>42.036505</td>\n",
       "      <td>-91.547915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Address   Latitude   Longitude\n",
       "0  Imperial Promenade,5675 E La Palma Avenue,Suit...  33.860970 -117.791142\n",
       "1   4509 Phoenix Avenue,Fort Smith, AR 72903-6005,US  35.338555  -94.383186\n",
       "2  3821 Lakewood Boulevard ,Ste. 101,Long Beach, ...  33.828598 -118.143035\n",
       "3  Station Park West,1060 West Park Lane,Suite 11...  40.983426 -111.907794\n",
       "4  6095 Carlson Way,Suite B,Marion, IA 52302-6651,US  42.036505  -91.547915"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loop through all locations, get geocode, if invalid then skip\n",
    "for location in locations:\n",
    "    latitude, longitude = None, None\n",
    "    base_url = 'https://maps.googleapis.com/maps/api/geocode/json'\n",
    "    endpoint = f'{base_url}?address={location}&key={api_key}'\n",
    "    r = requests.get(endpoint)\n",
    "    \n",
    "    # Invalid address\n",
    "    if r.status_code not in range(200, 299):\n",
    "        continue\n",
    "    \n",
    "    # Get geocode data\n",
    "    try:\n",
    "        results = r.json()['results'][0]\n",
    "        latitude = results['geometry']['location']['lat']\n",
    "        longitude = results['geometry']['location']['lng']\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "    # Append to dataframe\n",
    "    df.loc[len(df)] = [location, latitude, longitude]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4d70e9",
   "metadata": {},
   "source": [
    "# Export Dataframe to .csv File\n",
    "\n",
    "Export store locations datafraame to .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bec22d4e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.to_csv('../data/jersey_mikes_locations.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
